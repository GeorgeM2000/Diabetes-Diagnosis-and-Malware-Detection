{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FHKaF4wZsEQr",
        "Vq9VwFVInqbk",
        "lM1JGxSbZ9FI",
        "rmz6GPF07UMw"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeorgeM2000/Diabetes-Diagnosis-and-Malware-Detection/blob/main/Diabetes_Diagnosis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flVdFqudJiDE"
      },
      "source": [
        "# ***Libraries & Tools***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQA5Nz1FJNeL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "from itertools import product\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report, RocCurveDisplay, PrecisionRecallDisplay, make_scorer\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from collections import Counter\n",
        "from sklearn.compose import make_column_transformer, make_column_selector\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Data Overview and Exploration***"
      ],
      "metadata": {
        "id": "l2Kmlxgxhoe6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective is to predict based on diagnostic measurements whether a patient has diabetes."
      ],
      "metadata": {
        "id": "RHe3cr0xieKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Content***:\n",
        "Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n",
        "\n",
        "- Pregnancies: Number of times pregnant\n",
        "- Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
        "- BloodPressure: Diastolic blood pressure (mm Hg)\n",
        "- SkinThickness: Triceps skin fold thickness (mm)\n",
        "- Insulin: 2-Hour serum insulin (mu U/ml)\n",
        "- BMI: Body mass index (weight in kg/(height in m)^2)\n",
        "- DiabetesPedigreeFunction: Diabetes pedigree function\n",
        "- Age: Age (years)\n",
        "- Outcome: Class variable (0 or 1)\n"
      ],
      "metadata": {
        "id": "b0vFYKbwiigI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Relevant Information**:\n",
        "Several constraints were placed on the selection of these instances from a larger database.  In particular, all patients here are females at least 21 years old of Pima Indian heritage."
      ],
      "metadata": {
        "id": "jQVMrJo2i3VY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of Instances: 768\n",
        "Number of Attributes: 8 plus class\n",
        "For Each Attribute: (all numeric-valued)\n",
        "1. Number of times pregnant\n",
        "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
        "3. Diastolic blood pressure (mm Hg)\n",
        "4. Triceps skin fold thickness (mm)\n",
        "5. 2-Hour serum insulin (mu U/ml)\n",
        "6. Body mass index (weight in kg/(height in m)^2)\n",
        "7. Diabetes pedigree function\n",
        "8. Age (years)\n",
        "9. Class variable (0 or 1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8Qf5odtojKhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dd = pd.read_csv('diabetes.csv', delimiter=',')"
      ],
      "metadata": {
        "id": "y2jSM0Ekhyj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dd.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "QdUbb5vqiJWH",
        "outputId": "e30d95b9-8a7c-4abb-fff2-40c2375e8812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0            6      148             72             35        0  33.6   \n",
              "1            1       85             66             29        0  26.6   \n",
              "2            8      183             64              0        0  23.3   \n",
              "3            1       89             66             23       94  28.1   \n",
              "4            0      137             40             35      168  43.1   \n",
              "\n",
              "   DiabetesPedigreeFunction  Age  Outcome  \n",
              "0                     0.627   50        1  \n",
              "1                     0.351   31        0  \n",
              "2                     0.672   32        1  \n",
              "3                     0.167   21        0  \n",
              "4                     2.288   33        1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b0396e90-734f-4436-b261-a6c1c5f48bf1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0396e90-734f-4436-b261-a6c1c5f48bf1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b0396e90-734f-4436-b261-a6c1c5f48bf1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b0396e90-734f-4436-b261-a6c1c5f48bf1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0a85d524-2993-4492-a7ef-ec905d1ae261\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0a85d524-2993-4492-a7ef-ec905d1ae261')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0a85d524-2993-4492-a7ef-ec905d1ae261 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dd",
              "summary": "{\n  \"name\": \"dd\",\n  \"rows\": 768,\n  \"fields\": [\n    {\n      \"column\": \"Pregnancies\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 17,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          6,\n          1,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Glucose\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 31,\n        \"min\": 0,\n        \"max\": 199,\n        \"num_unique_values\": 136,\n        \"samples\": [\n          151,\n          101,\n          112\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BloodPressure\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 19,\n        \"min\": 0,\n        \"max\": 122,\n        \"num_unique_values\": 47,\n        \"samples\": [\n          86,\n          46,\n          85\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SkinThickness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 15,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 51,\n        \"samples\": [\n          7,\n          12,\n          48\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Insulin\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 115,\n        \"min\": 0,\n        \"max\": 846,\n        \"num_unique_values\": 186,\n        \"samples\": [\n          52,\n          41,\n          183\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BMI\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.884160320375446,\n        \"min\": 0.0,\n        \"max\": 67.1,\n        \"num_unique_values\": 248,\n        \"samples\": [\n          19.9,\n          31.0,\n          38.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DiabetesPedigreeFunction\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3313285950127749,\n        \"min\": 0.078,\n        \"max\": 2.42,\n        \"num_unique_values\": 517,\n        \"samples\": [\n          1.731,\n          0.426,\n          0.138\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": 21,\n        \"max\": 81,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          60,\n          47,\n          72\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Outcome\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dd.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNaDqp_5iKkt",
        "outputId": "f5a4646d-9e70-45b3-8e8f-0ae164279a63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 768 entries, 0 to 767\n",
            "Data columns (total 9 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   Pregnancies               768 non-null    int64  \n",
            " 1   Glucose                   768 non-null    int64  \n",
            " 2   BloodPressure             768 non-null    int64  \n",
            " 3   SkinThickness             768 non-null    int64  \n",
            " 4   Insulin                   768 non-null    int64  \n",
            " 5   BMI                       768 non-null    float64\n",
            " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
            " 7   Age                       768 non-null    int64  \n",
            " 8   Outcome                   768 non-null    int64  \n",
            "dtypes: float64(2), int64(7)\n",
            "memory usage: 54.1 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dd.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAR0U29Mjm1z",
        "outputId": "9562d783-e760-4556-efe3-6ebf15330faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# There are no duplicate samples\n",
        "dd.drop_duplicates().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gok1LeP1js-V",
        "outputId": "d3b76564-7eab-494b-9aee-3a06c653d9af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# There is a class imbalance\n",
        "dd['Outcome'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y02-4jRjwV6",
        "outputId": "5811632b-ea43-4c82-8d19-9220fca4e75e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    500\n",
              "1    268\n",
              "Name: Outcome, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# There are no null values\n",
        "dd.dropna().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-DHghXQj6fg",
        "outputId": "e5b87c04-87fb-4eee-a7ed-97123d2995e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Mining and Classification**\n",
        "**Note: Class 1 is interpreted as \"tested positive for diabetes\".**"
      ],
      "metadata": {
        "id": "aLdIwK2wkjoU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rows represent the actual classification and the columns the predicted classification.\n",
        "\n",
        "It is worse to classify a sample as \"tested negative\" when the actual class is \"tested positive\", than it is to to classify a sample as \"tested positive\" when the actual class is \"tested negative\"."
      ],
      "metadata": {
        "id": "ju2E2i4lk7Up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's define a custom cost matrix\n",
        "ORIGINAL_COST = [\n",
        "    [0, 0.25],\n",
        "    [1, 0]\n",
        "]\n",
        "\n",
        "\"\"\"\n",
        "                          Predicted Tested Negative (0)   Predicted Tested Positive (1)\n",
        "Actual Tested Negative (0)           TN                      FP\n",
        "Actual Tested Positive (1)           FN                      TP\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "PlxTCV9MltOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The counts for False Negatives (FN) must have a default cost of 1. This implies that if a sample is classified as a False Negative, it will be counted as if one sample is incorrect. While we could assign a higher cost to False Negatives, such a higher cost number is abstract. Therefore, we prefer to assign the default cost of 1.\n",
        "\n",
        "On the other hand, the counts for False Positives (FP) will have a cost of 0.25. This is because if a sample has \"tested negative\" for diabetes and the model incorrectly predicts it as \"tested positive\", then there will be a misclassification, but it's not as damaging as a False Negative. Therefore, we assign a lower cost of 0.25 to False Positives."
      ],
      "metadata": {
        "id": "ENF1d1a_m6ta"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiAoBvBJkH05"
      },
      "source": [
        "## ***Classification***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the dataset into a training set (75%) and a testing set (25%).\n",
        "\n"
      ],
      "metadata": {
        "id": "dapAk8iNYiOp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0xqE1S9ekV3"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(dd.drop(columns=['Outcome']), dd['Outcome'], test_size=0.25, random_state=42, stratify=dd['Outcome'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_names = [\n",
        "    'Random Forests', 'Linear SVM', 'Naive Bayes'\n",
        "]\n",
        "\n",
        "classifiers = [RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10, min_samples_split=10),\n",
        "               SVC(kernel='linear'),\n",
        "               GaussianNB()\n",
        "               ]"
      ],
      "metadata": {
        "id": "rHQDdBChsXq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Cost-based Evaluation***\n",
        "We run three classifiers, evaluate their performance, and calculate the total cost of their results."
      ],
      "metadata": {
        "id": "sZnQhx41ZcWz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YITndbffKbF",
        "outputId": "0d7256e6-751b-422a-adc2-deb169b92bd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forests\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.86      0.82       125\n",
            "           1       0.67      0.55      0.61        67\n",
            "\n",
            "    accuracy                           0.75       192\n",
            "   macro avg       0.73      0.70      0.71       192\n",
            "weighted avg       0.74      0.75      0.74       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[107  18]\n",
            " [ 30  37]]\n",
            "\n",
            "Total Cost: 34.5\n",
            "\n",
            "Linear SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.83      0.79       125\n",
            "           1       0.61      0.49      0.55        67\n",
            "\n",
            "    accuracy                           0.71       192\n",
            "   macro avg       0.68      0.66      0.67       192\n",
            "weighted avg       0.70      0.71      0.71       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[104  21]\n",
            " [ 34  33]]\n",
            "\n",
            "Total Cost: 39.25\n",
            "\n",
            "Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.77      0.78       125\n",
            "           1       0.59      0.61      0.60        67\n",
            "\n",
            "    accuracy                           0.71       192\n",
            "   macro avg       0.69      0.69      0.69       192\n",
            "weighted avg       0.72      0.71      0.71       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[96 29]\n",
            " [26 41]]\n",
            "\n",
            "Total Cost: 33.25\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for name, clf in zip(classifier_names, classifiers):\n",
        "  print(name)\n",
        "  clf.fit(x_train, y_train)\n",
        "  y_pred = clf.predict(x_test)\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  conf_m = confusion_matrix(y_test, y_pred)\n",
        "  print(f'Confusion Matrix: \\n{conf_m}\\n')\n",
        "  print(f'Total Cost: {np.sum(conf_m * ORIGINAL_COST)}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above results we can conclude the following:\n",
        "- The NB (Naive Bayes) has the lowest cost\n",
        "- The Naive Bayes (NB) classifier misclassifies 26 samples as \"tested negative\" when they are actually \"tested positive\". Although NB has the lowest count for false negatives (FN) among the algorithms, it also has the highest count for false positives (FP). Conversely, the Random Forest (RF) algorithm exhibits the lowest count for false positives, indicating its strength in correctly classifying samples that have \"tested negative\".\n",
        "- The performance of the SVM falls somewhere in between the other two algorithms\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PdtQh9LbfFJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Minimizing the Expected Cost***"
      ],
      "metadata": {
        "id": "FHKaF4wZsEQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_expected_cost_classifiers = [\n",
        "    RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10, min_samples_split=10),\n",
        "    SVC(kernel='linear', probability=True),\n",
        "    GaussianNB()\n",
        "]"
      ],
      "metadata": {
        "id": "RbeSOeNe8gjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cost minimization without probability calibration**"
      ],
      "metadata": {
        "id": "s_XZ7Nm05X7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, clf in zip(classifier_names, min_expected_cost_classifiers):\n",
        "  print(name)\n",
        "  model = clf.fit(x_train, y_train)\n",
        "  y_pred_prob = model.predict_proba(x_test)\n",
        "  y_pred = np.argmin(np.matmul(y_pred_prob, np.array(ORIGINAL_COST)), axis=1)\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  conf_m = confusion_matrix(y_test, y_pred)\n",
        "  print(f'Confusion Matrix: \\n{conf_m}\\n')\n",
        "  print(f'Total Cost: {np.sum(conf_m * ORIGINAL_COST)}\\n')"
      ],
      "metadata": {
        "id": "QYdkFw8ysOa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "775eb19d-4caf-4206-8b9e-0644827b97f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forests\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.53      0.67       125\n",
            "           1       0.50      0.90      0.65        67\n",
            "\n",
            "    accuracy                           0.66       192\n",
            "   macro avg       0.70      0.71      0.66       192\n",
            "weighted avg       0.76      0.66      0.66       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[66 59]\n",
            " [ 7 60]]\n",
            "\n",
            "Total Cost: 21.75\n",
            "\n",
            "Linear SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.58      0.72       125\n",
            "           1       0.54      0.93      0.69        67\n",
            "\n",
            "    accuracy                           0.70       192\n",
            "   macro avg       0.74      0.75      0.70       192\n",
            "weighted avg       0.80      0.70      0.71       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[73 52]\n",
            " [ 5 62]]\n",
            "\n",
            "Total Cost: 18.0\n",
            "\n",
            "Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.66      0.73       125\n",
            "           1       0.53      0.73      0.62        67\n",
            "\n",
            "    accuracy                           0.68       192\n",
            "   macro avg       0.68      0.69      0.67       192\n",
            "weighted avg       0.72      0.68      0.69       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[82 43]\n",
            " [18 49]]\n",
            "\n",
            "Total Cost: 28.75\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above results we can conclude that:\n",
        "- The SVM classifier exhibits superior performance with the lowest overall cost, the fewest false negatives (FN), and the second-lowest false positive (FP) count. Although the Naive Bayes (NB) classifier demonstrates a lower FP count compared to the SVM, its substantially higher FN count and cost render it impractical for classification without probability calibration.\n",
        "- The SVM classifier is the clear choice here."
      ],
      "metadata": {
        "id": "F3UbZuFHzha7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cost minimization with sigmoid calibration**"
      ],
      "metadata": {
        "id": "X2AT66P0Ctt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, clf in zip(classifier_names, min_expected_cost_classifiers):\n",
        "  print(name)\n",
        "  cc = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=3)\n",
        "  model = cc.fit(x_train, y_train)\n",
        "  y_pred_prob = model.predict_proba(x_test)\n",
        "  y_pred = np.argmin(np.matmul(y_pred_prob, np.array(ORIGINAL_COST)), axis=1)\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  conf_m = confusion_matrix(y_test, y_pred)\n",
        "  print(f'Confusion Matrix: \\n{conf_m}\\n')\n",
        "  print(f'Total Cost: {np.sum(conf_m * ORIGINAL_COST)}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqIFZBgtCq4B",
        "outputId": "6d161b65-d0fa-475d-d9f8-dd940d246d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forests\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.58      0.70       125\n",
            "           1       0.52      0.87      0.65        67\n",
            "\n",
            "    accuracy                           0.68       192\n",
            "   macro avg       0.71      0.72      0.68       192\n",
            "weighted avg       0.76      0.68      0.68       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[72 53]\n",
            " [ 9 58]]\n",
            "\n",
            "Total Cost: 22.25\n",
            "\n",
            "Linear SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.55      0.70       125\n",
            "           1       0.53      0.94      0.68        67\n",
            "\n",
            "    accuracy                           0.69       192\n",
            "   macro avg       0.74      0.75      0.69       192\n",
            "weighted avg       0.80      0.69      0.69       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[69 56]\n",
            " [ 4 63]]\n",
            "\n",
            "Total Cost: 18.0\n",
            "\n",
            "Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.61      0.71       125\n",
            "           1       0.53      0.82      0.64        67\n",
            "\n",
            "    accuracy                           0.68       192\n",
            "   macro avg       0.70      0.71      0.68       192\n",
            "weighted avg       0.75      0.68      0.69       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[76 49]\n",
            " [12 55]]\n",
            "\n",
            "Total Cost: 24.25\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above results we can conclude that\n",
        "- The SVM classifier emerges as the optimal method in this scenario, as it exhibits the lowest cost and (FN) count. Its (FP) count is slightly higher compared to the other two methods, albeit by a negligible margin."
      ],
      "metadata": {
        "id": "kUWYLIqMx36Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cost Minimization with isotonic calibration**"
      ],
      "metadata": {
        "id": "SXe8ARJYDbro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, clf in zip(classifier_names, min_expected_cost_classifiers):\n",
        "  print(name)\n",
        "  cc = CalibratedClassifierCV(clf, method=\"isotonic\", cv=3)\n",
        "  model = cc.fit(x_train, y_train)\n",
        "  y_pred_prob = model.predict_proba(x_test)\n",
        "  y_pred = np.argmin(np.matmul(y_pred_prob, np.array(ORIGINAL_COST)), axis=1)\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  conf_m = confusion_matrix(y_test, y_pred)\n",
        "  print(f'Confusion Matrix: \\n{conf_m}\\n')\n",
        "  print(f'Total Cost: {np.sum(conf_m * ORIGINAL_COST)}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_8fNMPIDiNL",
        "outputId": "5fc8a501-c494-4842-c807-d39db5d9d22f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forests\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.57      0.69       125\n",
            "           1       0.52      0.87      0.65        67\n",
            "\n",
            "    accuracy                           0.67       192\n",
            "   macro avg       0.70      0.72      0.67       192\n",
            "weighted avg       0.76      0.67      0.68       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[71 54]\n",
            " [ 9 58]]\n",
            "\n",
            "Total Cost: 22.5\n",
            "\n",
            "Linear SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.56      0.70       125\n",
            "           1       0.53      0.94      0.68        67\n",
            "\n",
            "    accuracy                           0.69       192\n",
            "   macro avg       0.74      0.75      0.69       192\n",
            "weighted avg       0.80      0.69      0.70       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[70 55]\n",
            " [ 4 63]]\n",
            "\n",
            "Total Cost: 17.75\n",
            "\n",
            "Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.50      0.64       125\n",
            "           1       0.49      0.90      0.63        67\n",
            "\n",
            "    accuracy                           0.64       192\n",
            "   macro avg       0.69      0.70      0.64       192\n",
            "weighted avg       0.76      0.64      0.64       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[62 63]\n",
            " [ 7 60]]\n",
            "\n",
            "Total Cost: 22.75\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above results we can conclude that:\n",
        "- The SVM classifier has once again the lowest cost outperforming the other methods."
      ],
      "metadata": {
        "id": "dYa0IUfcwskb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><b>The results have shown that the SVM classifier is the best choice when we are trying to minimize the expected cost.</b></h3>"
      ],
      "metadata": {
        "id": "k8OTTJASmZeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Rebalancing***"
      ],
      "metadata": {
        "id": "Vq9VwFVInqbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Undersampling**"
      ],
      "metadata": {
        "id": "XYxM8EG5oinE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(Counter(y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VISeQcllrbpt",
        "outputId": "cd717041-bbba-4f48-fe92-e275a93ff7e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({0: 375, 1: 201})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We reduce the size of the first class (\"Tested Negative\") with undersampling to be equal to the size of the second class (\"Tested Positive\")."
      ],
      "metadata": {
        "id": "STxaC6WGt28b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = RandomUnderSampler(sampling_strategy={0: 201, 1: 201}, random_state=42)\n",
        "x_rs, y_rs = sampler.fit_resample(x_train, y_train)\n",
        "\n",
        "for name, clf in zip(classifier_names, classifiers):\n",
        "  print(name)\n",
        "  clf.fit(x_rs, y_rs)\n",
        "  y_pred = clf.predict(x_test)\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  conf_m = confusion_matrix(y_test, y_pred)\n",
        "  print(f'Confusion Matrix: \\n{conf_m}\\n')\n",
        "  print(f'Total Cost: {np.sum(conf_m * ORIGINAL_COST)}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxrKBnSSn6m-",
        "outputId": "c70d1a5a-69bd-4dfd-9151-33761c1b2173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forests\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.74      0.80       125\n",
            "           1       0.62      0.81      0.70        67\n",
            "\n",
            "    accuracy                           0.76       192\n",
            "   macro avg       0.75      0.77      0.75       192\n",
            "weighted avg       0.79      0.76      0.77       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[92 33]\n",
            " [13 54]]\n",
            "\n",
            "Total Cost: 21.25\n",
            "\n",
            "Linear SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.79      0.80       125\n",
            "           1       0.63      0.66      0.64        67\n",
            "\n",
            "    accuracy                           0.74       192\n",
            "   macro avg       0.72      0.72      0.72       192\n",
            "weighted avg       0.75      0.74      0.75       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[99 26]\n",
            " [23 44]]\n",
            "\n",
            "Total Cost: 29.5\n",
            "\n",
            "Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.75      0.78       125\n",
            "           1       0.59      0.66      0.62        67\n",
            "\n",
            "    accuracy                           0.72       192\n",
            "   macro avg       0.70      0.70      0.70       192\n",
            "weighted avg       0.73      0.72      0.72       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[94 31]\n",
            " [23 44]]\n",
            "\n",
            "Total Cost: 30.75\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above results we can say that:\n",
        "- The Random Forests classifier has the lowest cost and outperforms both the SVM and the Naive Bayes classifiers. It exhibits the lowest count of (FN), and while it has the highest count of (FP), the disparity is minimal."
      ],
      "metadata": {
        "id": "VSM7FibmnfiZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Oversampling**:\n",
        "We increase the size of the minority class (\"Tested Positive\") with oversampling to be equal to the size of the majority class (\"Tested Negative\")."
      ],
      "metadata": {
        "id": "cL8yn9aOtwJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = RandomOverSampler(sampling_strategy={0: 375, 1: 375}, random_state=42)\n",
        "x_rs, y_rs = sampler.fit_resample(x_train, y_train)\n",
        "\n",
        "for name, clf in zip(classifier_names, classifiers):\n",
        "  print(name)\n",
        "  clf.fit(x_rs, y_rs)\n",
        "  y_pred = clf.predict(x_test)\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  conf_m = confusion_matrix(y_test, y_pred)\n",
        "  print(f'Confusion Matrix: \\n{conf_m}\\n')\n",
        "  print(f'Total Cost: {np.sum(conf_m * ORIGINAL_COST)}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8YMPUqYtYp9",
        "outputId": "d29e2c0c-c4ae-49d2-e630-e1a2140074de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forests\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.78      0.80       125\n",
            "           1       0.63      0.69      0.66        67\n",
            "\n",
            "    accuracy                           0.75       192\n",
            "   macro avg       0.73      0.74      0.73       192\n",
            "weighted avg       0.76      0.75      0.75       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[98 27]\n",
            " [21 46]]\n",
            "\n",
            "Total Cost: 27.75\n",
            "\n",
            "Linear SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.78      0.80       125\n",
            "           1       0.62      0.66      0.64        67\n",
            "\n",
            "    accuracy                           0.74       192\n",
            "   macro avg       0.71      0.72      0.72       192\n",
            "weighted avg       0.74      0.74      0.74       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[98 27]\n",
            " [23 44]]\n",
            "\n",
            "Total Cost: 29.75\n",
            "\n",
            "Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.74      0.78       125\n",
            "           1       0.58      0.67      0.62        67\n",
            "\n",
            "    accuracy                           0.72       192\n",
            "   macro avg       0.70      0.71      0.70       192\n",
            "weighted avg       0.73      0.72      0.72       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[93 32]\n",
            " [22 45]]\n",
            "\n",
            "Total Cost: 30.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the results of using undersampling and oversampling we can conclude that:\n",
        "- The counts of the False Negatives (FN) are relatively high while the counts of the False Positives (FP) are low compaired to the True Negatives (TN). This isn't ideal because we want to keep the (FN) count as low as possible."
      ],
      "metadata": {
        "id": "G9pW5w5f-CGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combination of undersampling and oversampling**: We reduce the number of samples in the majority class and increase the number of samples in the minority class."
      ],
      "metadata": {
        "id": "Adzl66sau9d_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = RandomUnderSampler(sampling_strategy={0: 201, 1: 201}, random_state=42)\n",
        "x_rs, y_rs = sampler.fit_resample(x_train, y_train)\n",
        "sampler = RandomOverSampler(sampling_strategy={0: 201, 1: 375}, random_state=42)\n",
        "x_rs, y_rs = sampler.fit_resample(x_rs, y_rs)\n",
        "\n",
        "for name, clf in zip(classifier_names, classifiers):\n",
        "  print(name)\n",
        "  clf.fit(x_rs, y_rs)\n",
        "  y_pred = clf.predict(x_test)\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  conf_m = confusion_matrix(y_test, y_pred)\n",
        "  print(f'Confusion Matrix: \\n{conf_m}\\n')\n",
        "  print(f'Total Cost: {np.sum(conf_m * ORIGINAL_COST)}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMRlZ6ZntrCp",
        "outputId": "4cb71b4f-9c41-44c3-89f6-4ce2a8c4fca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (375) in class 1 will be larger than the number of samples in the majority class (class #0 -> 201)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forests\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.68      0.78       125\n",
            "           1       0.60      0.88      0.71        67\n",
            "\n",
            "    accuracy                           0.75       192\n",
            "   macro avg       0.75      0.78      0.75       192\n",
            "weighted avg       0.80      0.75      0.76       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[85 40]\n",
            " [ 8 59]]\n",
            "\n",
            "Total Cost: 18.0\n",
            "\n",
            "Linear SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.62      0.74       125\n",
            "           1       0.55      0.87      0.67        67\n",
            "\n",
            "    accuracy                           0.71       192\n",
            "   macro avg       0.72      0.74      0.71       192\n",
            "weighted avg       0.78      0.71      0.71       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[78 47]\n",
            " [ 9 58]]\n",
            "\n",
            "Total Cost: 20.75\n",
            "\n",
            "Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.69      0.75       125\n",
            "           1       0.56      0.73      0.63        67\n",
            "\n",
            "    accuracy                           0.70       192\n",
            "   macro avg       0.69      0.71      0.69       192\n",
            "weighted avg       0.73      0.70      0.71       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[86 39]\n",
            " [18 49]]\n",
            "\n",
            "Total Cost: 27.75\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the results of combining undersampling and oversampling we can say that:\n",
        "- The Random Forests (RF) and SVM classifiers have similar performance but the RF is sligtly better because it has a lower (FN) count.\n",
        "- The NB has a high (FN) count which is something we want to avoid."
      ],
      "metadata": {
        "id": "Oz-HtUet_Bbi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><b>The two contenders are the Random Forests classifier with undersampling and the Random Forests with both undersampling and oversampling. The former is suitable when false positives (FP) are important but not as critical as false negatives (FN), while the latter is preferable when minimizing false negatives (FN) is our primary objective.</b></h3>"
      ],
      "metadata": {
        "id": "Ur2HfCH3rlqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Cost-based Evaluation using Sample Weights***\n",
        "Now we want to use sample weights to evaluate the cost of the classification."
      ],
      "metadata": {
        "id": "lM1JGxSbZ9FI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we assign weights to each training instance. The weights are defined using the default cost matrix."
      ],
      "metadata": {
        "id": "RFnwfx4pc9FZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = np.zeros(y_train.shape[0])\n",
        "weights[np.where(y_train == 0)] = 0.25;\n",
        "weights[np.where(y_train == 1)] = 1;"
      ],
      "metadata": {
        "id": "FiNS7f8ZeCUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, clf in zip(classifier_names, classifiers):\n",
        "  print(name)\n",
        "  clf.fit(x_train, y_train, weights)\n",
        "  y_pred = clf.predict(x_test)\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  conf_m = confusion_matrix(y_test, y_pred)\n",
        "  print(f'Confusion Matrix: \\n{conf_m}\\n')\n",
        "  print(f'Total Cost: {np.sum(conf_m * ORIGINAL_COST)}\\n')"
      ],
      "metadata": {
        "id": "-l8Zb6dMaCOr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "998eef16-df8d-4d4a-c11f-90d2ee5e9c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forests\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.74      0.80       125\n",
            "           1       0.62      0.79      0.69        67\n",
            "\n",
            "    accuracy                           0.76       192\n",
            "   macro avg       0.74      0.76      0.74       192\n",
            "weighted avg       0.78      0.76      0.76       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[92 33]\n",
            " [14 53]]\n",
            "\n",
            "Total Cost: 22.25\n",
            "\n",
            "Linear SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.61      0.73       125\n",
            "           1       0.55      0.88      0.67        67\n",
            "\n",
            "    accuracy                           0.70       192\n",
            "   macro avg       0.73      0.74      0.70       192\n",
            "weighted avg       0.78      0.70      0.71       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[76 49]\n",
            " [ 8 59]]\n",
            "\n",
            "Total Cost: 20.25\n",
            "\n",
            "Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.66      0.73       125\n",
            "           1       0.53      0.73      0.62        67\n",
            "\n",
            "    accuracy                           0.68       192\n",
            "   macro avg       0.68      0.69      0.67       192\n",
            "weighted avg       0.72      0.68      0.69       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[82 43]\n",
            " [18 49]]\n",
            "\n",
            "Total Cost: 28.75\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above results we can conclude the following:\n",
        "- The Support Vector Machine (SVM) demonstrates a favorable outcome with minimal (FN); however, it concurrently exhibits the highest count of (FP). This performance might be less than ideal because while the model makes fewer errors in categorizing samples as (FN), it overlooks a significant number of samples by misclassifying them as (FP).\n",
        "- The Random Forest (RF) algorithm is worse that SVM with regards to the (FN) count, but better in (FP).\n",
        "- In this scenario, the SVM is the preferred choice if minimizing false negatives (FN) is of utmost importance, even at the expense of potentially higher false positives (FP). On the other hand, the RF classifier is favored if both false positives (FP) and false negatives (FN) are critical, with a slightly higher priority given to minimizing false negatives.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wVEQFhrSnSu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Combining Techniques***\n",
        "We can experiment by combining the above techniques and methods."
      ],
      "metadata": {
        "id": "rmz6GPF07UMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the results so far, our goal is to keep the False Negatives (FN) as low as posible while also maintaining a fairly low count of False Positives (FP). In other words, we aim to avoid misclassifying individuals who test positive for diabetes as \"Tested Negative,\" and likewise, we strive to prevent confusion by inaccurately indicating that individuals have tested positive for diabetes when they have actually tested negative. How can we achieve this goal?"
      ],
      "metadata": {
        "id": "oj6f_HpP8cJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1)** We can conduct experiments by assigning weights to the training examples and oversample the minority class."
      ],
      "metadata": {
        "id": "iyHDGvbPZqB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = RandomOverSampler(sampling_strategy={0: 375, 1: 375}, random_state=42)\n",
        "x_rs, y_rs = sampler.fit_resample(x_train, y_train)"
      ],
      "metadata": {
        "id": "91zXGJO9aNns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = np.zeros(y_rs.shape[0])\n",
        "weights[np.where(y_rs == 0)] = 0.25;\n",
        "weights[np.where(y_rs == 1)] = 1;"
      ],
      "metadata": {
        "id": "qqWk5G-OaDyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, clf in zip(classifier_names, classifiers):\n",
        "  print(name)\n",
        "  clf.fit(x_rs, y_rs, weights)\n",
        "  y_pred = clf.predict(x_test)\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  conf_m = confusion_matrix(y_test, y_pred)\n",
        "  print(f'Confusion Matrix: \\n{conf_m}\\n')\n",
        "  print(f'Total Cost: {np.sum(conf_m * ORIGINAL_COST)}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QWzspSmaRLt",
        "outputId": "2552c0a3-2c85-4840-f2e0-22981b1669af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forests\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.69      0.78       125\n",
            "           1       0.59      0.85      0.70        67\n",
            "\n",
            "    accuracy                           0.74       192\n",
            "   macro avg       0.74      0.77      0.74       192\n",
            "weighted avg       0.79      0.74      0.75       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[86 39]\n",
            " [10 57]]\n",
            "\n",
            "Total Cost: 19.75\n",
            "\n",
            "Linear SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.38      0.55       125\n",
            "           1       0.46      0.99      0.63        67\n",
            "\n",
            "    accuracy                           0.59       192\n",
            "   macro avg       0.72      0.68      0.59       192\n",
            "weighted avg       0.80      0.59      0.58       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[48 77]\n",
            " [ 1 66]]\n",
            "\n",
            "Total Cost: 20.25\n",
            "\n",
            "Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.57      0.69       125\n",
            "           1       0.51      0.84      0.63        67\n",
            "\n",
            "    accuracy                           0.66       192\n",
            "   macro avg       0.69      0.70      0.66       192\n",
            "weighted avg       0.74      0.66      0.67       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[71 54]\n",
            " [11 56]]\n",
            "\n",
            "Total Cost: 24.5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above results we can say that:\n",
        "- The Random Forests (RF) classifier outperforms the SVM and NB classfiers because both the (FN) and the (FP) counts are relatively low compared to the (TN) and (TP) counts.\n",
        "- The SVM has the lowest (FN) count but at the same time the highest (FP) count which means that our model will cause confusion as to who has actually tested negative for diabetes."
      ],
      "metadata": {
        "id": "XLH6MG_83AMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2)** We can oversample the minority class and use probability calibration."
      ],
      "metadata": {
        "id": "d49R9pxBZyZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = RandomOverSampler(sampling_strategy={0: 375, 1: 375}, random_state=42)\n",
        "x_rs, y_rs = sampler.fit_resample(x_train, y_train)"
      ],
      "metadata": {
        "id": "s1ddftm1D3of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, clf in zip(classifier_names, min_expected_cost_classifiers):\n",
        "  print(name)\n",
        "  cc = CalibratedClassifierCV(clf, method=\"isotonic\", cv=3)\n",
        "  model = cc.fit(x_rs, y_rs)\n",
        "  y_pred_prob = model.predict_proba(x_test)\n",
        "  y_pred = np.argmin(np.matmul(y_pred_prob, np.array(ORIGINAL_COST)), axis=1)\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  conf_m = confusion_matrix(y_test, y_pred)\n",
        "  print(f'Confusion Matrix: \\n{conf_m}\\n')\n",
        "  print(f'Total Cost: {np.sum(conf_m * ORIGINAL_COST)}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQx-6afx0t38",
        "outputId": "d901c71e-dc6a-4fb1-b30e-75afc3bb9022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forests\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.66      0.76       125\n",
            "           1       0.57      0.85      0.68        67\n",
            "\n",
            "    accuracy                           0.72       192\n",
            "   macro avg       0.73      0.75      0.72       192\n",
            "weighted avg       0.78      0.72      0.73       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[82 43]\n",
            " [10 57]]\n",
            "\n",
            "Total Cost: 20.75\n",
            "\n",
            "Linear SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.46      0.62       125\n",
            "           1       0.49      0.99      0.66        67\n",
            "\n",
            "    accuracy                           0.64       192\n",
            "   macro avg       0.74      0.72      0.64       192\n",
            "weighted avg       0.81      0.64      0.63       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[57 68]\n",
            " [ 1 66]]\n",
            "\n",
            "Total Cost: 18.0\n",
            "\n",
            "Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.36      0.52       125\n",
            "           1       0.45      0.97      0.61        67\n",
            "\n",
            "    accuracy                           0.57       192\n",
            "   macro avg       0.70      0.67      0.57       192\n",
            "weighted avg       0.78      0.57      0.55       192\n",
            "\n",
            "Confusion Matrix: \n",
            "[[45 80]\n",
            " [ 2 65]]\n",
            "\n",
            "Total Cost: 22.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, none of the methods achieve our objective, rendering them unsuitable for this classification task.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7Np894fDeADL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b>Comparing the Random Forests (RF) classifier with oversampling and weights against the one with a combination of undersampling and oversampling, we can confidently assert that the Random Forests (RF) classifier, using a combination of undersampling and oversampling, is the optimal choice for this classification task.</b></h2>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BmIpSjQu5DKp"
      }
    }
  ]
}